{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ECON 428 PSET 9<h1>\n",
    "<h3>Mark Viti and Marcus Lisman<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Derivation of $\\frac{\\partial h^l}{\\partial b^l}$\n",
    "\n",
    "In this part, we need to show that:\n",
    "$$\n",
    "\\frac{\\partial h^l}{\\partial b^l} = (f^l)'(W^lh^{l-1} + b^l)\n",
    "$$\n",
    "\n",
    "To begin, let's recall the general form for a layer in a neural network:\n",
    "$$\n",
    "h^l = f^l(W^lh^{l-1} + b^l)\n",
    "$$\n",
    "where:\n",
    "- $h^l$ is the output of the $l$-th layer,\n",
    "- $f^l$ is the activation function of the $l$-th layer,\n",
    "- $W^l$ is the weight matrix connecting layer $l-1$ to layer $l$,\n",
    "- $b^l$ is the bias vector for layer $l$.\n",
    "\n",
    "Given this setup, $h^l$ is a function of $W^l$, $h^{l-1}$, and $b^l$. To find $\\frac{\\partial h^l}{\\partial b^l}$, we use the fact that the only term in the expression $W^lh^{l-1} + b^l$ that directly depends on $b^l$ is $b^l$ itself. Hence, using the chain rule, we get:\n",
    "$$\n",
    "\\frac{\\partial h^l}{\\partial b^l} = \\frac{\\partial f^l}{\\partial (W^lh^{l-1} + b^l)} \\cdot \\frac{\\partial (W^lh^{l-1} + b^l)}{\\partial b^l}\n",
    "$$\n",
    "The derivative of $f^l$ with respect to its input is $(f^l)'(W^lh^{l-1} + b^l)$, and the derivative of $W^lh^{l-1} + b^l$ with respect to $b^l$ is 1 since $W^lh^{l-1}$ is constant with respect to $b^l$. This simplifies to:\n",
    "$$\n",
    "\\frac{\\partial h^l}{\\partial b^l} = (f^l)'(W^lh^{l-1} + b^l)\n",
    "$$\n",
    "\n",
    "#### Part 2: Computation of $\\frac{\\partial L}{\\partial b^l}$ Using Backpropagation\n",
    "\n",
    "To find $\\frac{\\partial L}{\\partial b^l}$ where $L$ is the loss function, we again utilize the chain rule. In backpropagation, we often propagate gradients backward from the output towards the input. Using the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^l} = \\frac{\\partial L}{\\partial h^l} \\cdot \\frac{\\partial h^l}{\\partial b^l}\n",
    "$$\n",
    "From part 1, we know that $\\frac{\\partial h^l}{\\partial b^l} = (f^l)'(W^lh^{l-1} + b^l)$. To express $\\frac{\\partial L}{\\partial h^l}$ in a form suitable for backpropagation, note that:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h^l} = \\sum_k \\frac{\\partial L}{\\partial h^{l+1}_k} \\cdot \\frac{\\partial h^{l+1}_k}{\\partial h^l}\n",
    "$$\n",
    "where the summation is over all units $k$ in layer $l+1$. This expression can be thought of as the \"backpropagated error\" from layer $l+1$ to layer $l$. The term $\\frac{\\partial h^{l+1}_k}{\\partial h^l}$ can be derived similarly by considering the impact of each unit in layer $l$ on each unit in layer $l+1$.\n",
    "\n",
    "In practice, the computation of $\\frac{\\partial L}{\\partial b^l}$ during backpropagation involves:\n",
    "1. Calculating the gradient of the loss with respect to the outputs of layer $l$ (i.e., $ \\frac{\\partial L}{\\partial h^l}$).\n",
    "2. Multiplying this by the gradient of the outputs of layer $l$ with respect to the biases of layer $l$ (i.e., $(f^l)'(W^lh^{l-1} + b^l)$).\n",
    "\n",
    "This results in an efficient way to compute gradients layer-by-layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss Value: 0.3548924095009708\n",
      "Cross Entropy Loss Value: 1.02901365677428\n",
      "MSE derivative: [-1.85089871 -0.02622613 -0.45551046  0.39743136 -1.98895577]\n",
      "Cross Entropy Derivative: [ -13.41370041   -1.0132873    -1.29492622    1.24799647 -181.08996561]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded CSV file\n",
    "data_path = 'ps9.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "def mse_loss(y_true, y_pred, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Calculate the MSE loss with L2 regularization.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (np.array): Actual values.\n",
    "        y_pred (np.array): Predicted values.\n",
    "        theta (np.array): Model parameters.\n",
    "        lambda_reg (float): Regularization strength.\n",
    "        \n",
    "    Returns:\n",
    "        float: MSE loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    l2_penalty = lambda_reg * np.sum(theta ** 2)\n",
    "    return mse + l2_penalty\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Calculate the Cross-Entropy loss with L2 regularization.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (np.array): Actual values (binary labels).\n",
    "        y_pred (np.array): Predicted probabilities.\n",
    "        theta (np.array): Model parameters.\n",
    "        lambda_reg (float): Regularization strength.\n",
    "        \n",
    "    Returns:\n",
    "        float: Cross-Entropy loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    # Avoid division by zero and log of zero by clipping values\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    cross_entropy = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    l2_penalty = lambda_reg * np.sum(theta ** 2)\n",
    "    return cross_entropy + l2_penalty\n",
    "\n",
    "# Derivatives of the loss functions\n",
    "def derivative_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of MSE loss with respect to predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (np.array): Actual values.\n",
    "        y_pred (np.array): Predicted values.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Derivative of MSE.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def derivative_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of Cross-Entropy loss with respect to predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (np.array): Actual values (binary labels).\n",
    "        y_pred (np.array): Predicted probabilities.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Derivative of Cross-Entropy.\n",
    "    \"\"\"\n",
    "    # Avoid division by zero and log of zero by clipping values\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "# Example usage\n",
    "y_true = data['y'].values\n",
    "y_pred = np.random.random(size=len(data))  # random predictions for example\n",
    "theta = np.random.random(size=5)  # random model parameters\n",
    "lambda_reg = 0.01  # regularization strength\n",
    "\n",
    "# Calculate losses\n",
    "mse_loss_value = mse_loss(y_true, y_pred, theta, lambda_reg)\n",
    "cross_entropy_loss_value = cross_entropy_loss(y_true, y_pred, theta, lambda_reg)\n",
    "\n",
    "# Calculate derivatives\n",
    "mse_derivative = derivative_mse(y_true, y_pred)\n",
    "cross_entropy_derivative = derivative_cross_entropy(y_true, y_pred)\n",
    "\n",
    "print(\"MSE Loss Value:\", mse_loss_value)\n",
    "print(\"Cross Entropy Loss Value:\", cross_entropy_loss_value)\n",
    "print(\"MSE derivative:\", mse_derivative[:5])\n",
    "print(\"Cross Entropy Derivative:\", cross_entropy_derivative[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Values: [[0.71655596 0.84853901 0.73976027 0.75567192 0.67431597]\n",
      " [0.6328661  0.53219916 0.37372385 0.48488821 0.45987194]\n",
      " [0.76741011 0.68166821 0.63514845 0.80159893 0.65398492]\n",
      " ...\n",
      " [0.29676822 0.26650994 0.25711134 0.23311642 0.21822044]\n",
      " [0.71180585 0.62386037 0.57355384 0.66554611 0.55629567]\n",
      " [0.70673125 0.402548   0.52776813 0.45632282 0.3512221 ]]\n",
      "ReLu Values: [[0.92744164 1.72318792 1.04472291 1.1290954  0.72777112]\n",
      " [0.54453225 0.12897515 0.         0.         0.        ]\n",
      " [1.1937446  0.76144884 0.55436818 1.39631779 0.63660195]\n",
      " ...\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.90417082 0.50596617 0.29636569 0.68810888 0.22614153]\n",
      " [0.87956102 0.         0.11118692 0.         0.        ]]\n",
      "Sigmoid Derivative Values: [[0.20310352 0.12852056 0.19251501 0.18463187 0.21961394]\n",
      " [0.2323466  0.24896321 0.23405433 0.24977163 0.24838974]\n",
      " [0.17849183 0.21699666 0.2317349  0.15903809 0.22628864]\n",
      " ...\n",
      " [0.20869684 0.19548239 0.1910051  0.17877315 0.17060028]\n",
      " [0.20513828 0.23465861 0.24458983 0.22259449 0.2468308 ]\n",
      " [0.20726219 0.24050311 0.24922893 0.2480923  0.22786514]]\n",
      "ReLu Derivative Values: [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reload the data from the uploaded CSV file\n",
    "data_path_reloaded = 'ps9.csv'\n",
    "data_reloaded = pd.read_csv(data_path_reloaded)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid activation function. \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    \"\"\" Derivative of the sigmoid function. \"\"\"\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\" ReLU activation function. \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    \"\"\" Derivative of the ReLU function. \"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Extract feature columns for activation functions\n",
    "features = data_reloaded[['x1', 'x2', 'x3', 'x4', 'x5']].values\n",
    "\n",
    "# Apply Sigmoid and ReLU functions to the features\n",
    "sigmoid_values = sigmoid(features)\n",
    "relu_values = relu(features)\n",
    "\n",
    "# Calculate derivatives of Sigmoid and ReLU for the features\n",
    "sigmoid_derivatives = derivative_sigmoid(features)\n",
    "relu_derivatives = derivative_relu(features)\n",
    "\n",
    "print(\"Sigmoid Values:\", sigmoid_values)\n",
    "print(\"ReLu Values:\", relu_values)\n",
    "print(\"Sigmoid Derivative Values:\", sigmoid_derivatives)\n",
    "print(\"ReLu Derivative Values:\", relu_derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 - Weights Gradient:\n",
      "[[-2.45168344e-07 -6.86940689e-07  3.72276653e-10  1.58705359e-06\n",
      "   1.16713690e-07]\n",
      " [-6.69223131e-07 -7.39021710e-07  6.25554916e-08  1.09194465e-06\n",
      "   8.05035987e-09]\n",
      " [ 4.64984202e-07 -3.87214691e-07 -1.14390639e-08  6.66634559e-07\n",
      "   1.02333169e-07]\n",
      " [ 6.37024868e-07 -1.03860797e-06  4.16636682e-08  9.83492984e-07\n",
      "   2.10121638e-07]\n",
      " [ 1.06710208e-06 -1.57054225e-06  7.23470489e-08  4.10993109e-07\n",
      "   2.65212679e-07]]\n",
      "Biases Gradient:\n",
      "[ 2.19892270e-06 -2.34835484e-06  5.94817403e-08  1.06963767e-06\n",
      "  6.10146820e-07]\n",
      "\n",
      "Layer 2 - Weights Gradient:\n",
      "[[ 1.71334173e-06  5.00164780e-07 -1.99821881e-07 -6.54347607e-07]\n",
      " [ 1.03741317e-06  3.02694863e-07 -1.20948699e-07 -3.95900405e-07]\n",
      " [ 7.10238723e-09  2.07183277e-09 -8.27869258e-10 -2.71047982e-09]\n",
      " [ 9.61271975e-07  2.80546719e-07 -1.12094287e-07 -3.66621832e-07]\n",
      " [ 2.61776071e-06  7.64001885e-07 -3.05245255e-07 -9.99357800e-07]]\n",
      "Biases Gradient:\n",
      "[ 2.79413030e-05  8.15376389e-06 -3.25785431e-06 -1.06643937e-05]\n",
      "\n",
      "Layer 3 - Weights Gradient:\n",
      "[[ 0.00000000e+00  2.74198630e-04 -8.11997924e-05]\n",
      " [ 0.00000000e+00  2.60402447e-04 -7.71142614e-05]\n",
      " [ 0.00000000e+00  2.71887952e-04 -8.05155210e-05]\n",
      " [ 0.00000000e+00  2.53263814e-04 -7.50002629e-05]]\n",
      "Biases Gradient:\n",
      "[ 0.          0.00053532 -0.00015853]\n",
      "\n",
      "Layer 4 - Weights Gradient:\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-3.50218753e-04  7.11223150e-04 -1.58046072e-03 -1.52153718e-03\n",
      "  -9.18031239e-04]\n",
      " [-4.97833220e-05  1.01099816e-04 -2.24661255e-04 -2.16285321e-04\n",
      "  -1.30497424e-04]]\n",
      "Biases Gradient:\n",
      "[-0.00146811  0.00298144 -0.00662527 -0.00637826 -0.00384837]\n",
      "\n",
      "Layer 5 - Weights Gradient:\n",
      "[[-0.11146221]\n",
      " [-0.11205738]\n",
      " [-0.11518496]\n",
      " [-0.10849991]\n",
      " [-0.11179852]]\n",
      "Biases Gradient:\n",
      "[-0.21990205]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_layer_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    weights = data.iloc[:, :-1].values.T\n",
    "    biases = data.iloc[:, -1].values\n",
    "    return weights, biases\n",
    "\n",
    "class neural_net:\n",
    "    def __init__(self, layers_config, activation_funcs):\n",
    "        self.layers = layers_config\n",
    "        self.activations = activation_funcs\n",
    "        self.activations_funcs = {\n",
    "            \"ReLU\": (lambda x: np.maximum(0, x), lambda x: (x > 0).astype(float)),\n",
    "            \"sigmoid\": (lambda x: 1 / (1 + np.exp(-x)), lambda x: x * (1 - x))\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = {'A': [x]}  # Cache to store layer inputs, activations, and linear transforms\n",
    "        A = x\n",
    "        for i, (weights, biases) in enumerate(self.layers):\n",
    "            Z = np.dot(A, weights) + biases\n",
    "            activation_func = self.activations_funcs[self.activations[i]][0]\n",
    "            A = activation_func(Z)\n",
    "            self.cache['A'].append(A)\n",
    "        return A\n",
    "\n",
    "    def backward(self, y_true, output, loss_type='mse'):\n",
    "        derivatives = []\n",
    "        error = output - y_true if loss_type == 'mse' else (output - y_true) / (output * (1 - output))\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            A_prev = self.cache['A'][i]\n",
    "            dA = error * self.activations_funcs[self.activations[i]][1](self.cache['A'][i+1])\n",
    "            dW = np.dot(A_prev.T, dA) / A_prev.shape[0]\n",
    "            dB = np.sum(dA, axis=0) / A_prev.shape[0]\n",
    "            derivatives.insert(0, (dW, dB))\n",
    "            if i > 0:  # Propagate error backward\n",
    "                error = np.dot(dA, self.layers[i][0].T)\n",
    "        \n",
    "        return derivatives\n",
    "\n",
    "# Load data for the network\n",
    "data_path = 'ps9.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "x_sample = data.drop('y', axis=1).values  # Feature matrix\n",
    "y_sample = data['y'].values.reshape(-1, 1)  # Target values, reshaped for network output\n",
    "\n",
    "# Load weights and biases for all layers\n",
    "layers_config = [load_layer_data(f'layer{i+1}.csv') for i in range(5)]\n",
    "activations = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"]\n",
    "\n",
    "# Initialize the network\n",
    "network = neural_net(layers_config, activations)\n",
    "\n",
    "# Forward pass\n",
    "output = network.forward(x_sample)\n",
    "\n",
    "# Backward pass\n",
    "derivatives = network.backward(y_sample, output, 'mse')\n",
    "\n",
    "# Output derivatives for each layer\n",
    "for i, (dW, dB) in enumerate(derivatives):\n",
    "    print(f\"Layer {i+1} - Weights Gradient:\\n{dW}\\nBiases Gradient:\\n{dB}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Gradients (First Hidden Layer): Weights\n",
      " [[ 4.47227732e-06 -4.90277833e-06  0.00000000e+00  0.00000000e+00\n",
      "   9.10898171e-07]\n",
      " [-8.94455463e-06  9.80555665e-06  0.00000000e+00  0.00000000e+00\n",
      "  -1.82179634e-06]\n",
      " [ 1.34168319e-05 -1.47083350e-05  0.00000000e+00  0.00000000e+00\n",
      "   2.73269451e-06]\n",
      " [-1.78891093e-05  1.96111133e-05  0.00000000e+00  0.00000000e+00\n",
      "  -3.64359268e-06]\n",
      " [ 2.23613866e-05 -2.45138916e-05  0.00000000e+00  0.00000000e+00\n",
      "   4.55449085e-06]]\n",
      "MSE Gradients (First Hidden Layer): Biases\n",
      " [ 4.47227732e-05 -4.90277833e-05  0.00000000e+00  0.00000000e+00\n",
      "  9.10898171e-06]\n",
      "Cross-Entropy Gradients (First Hidden Layer): Weights\n",
      " [[ 9.40423940e-07 -1.03094906e-06  0.00000000e+00  0.00000000e+00\n",
      "   1.91542336e-07]\n",
      " [-1.88084788e-06  2.06189813e-06  0.00000000e+00  0.00000000e+00\n",
      "  -3.83084673e-07]\n",
      " [ 2.82127182e-06 -3.09284719e-06  0.00000000e+00  0.00000000e+00\n",
      "   5.74627009e-07]\n",
      " [-3.76169576e-06  4.12379625e-06  0.00000000e+00  0.00000000e+00\n",
      "  -7.66169346e-07]\n",
      " [ 4.70211970e-06 -5.15474532e-06  0.00000000e+00  0.00000000e+00\n",
      "   9.57711682e-07]]\n",
      "Cross-Entropy Gradients (First Hidden Layer): Biases\n",
      " [ 9.40423940e-06 -1.03094906e-05  0.00000000e+00  0.00000000e+00\n",
      "  1.91542336e-06]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # x assumed to be sigmoid output\n",
    "\n",
    "# Load layer data\n",
    "def load_layer_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    weights = data.iloc[:, :-1].values.T\n",
    "    biases = data.iloc[:, -1].values\n",
    "    return weights, biases\n",
    "\n",
    "class neural_net:\n",
    "    def __init__(self, nh, activations, layer_files):\n",
    "        self.nh = nh\n",
    "        self.activations = activations\n",
    "        self.layers = []\n",
    "        for i in range(len(nh)):\n",
    "            weights, biases = load_layer_data(layer_files[i])\n",
    "            self.layers.append((weights, biases))\n",
    "        self.activations_funcs = {'ReLU': (relu, relu_derivative), 'sigmoid': (sigmoid, sigmoid_derivative)}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = {'A': [x]}  # Cache to store inputs and activations\n",
    "        for i, (weights, biases) in enumerate(self.layers):\n",
    "            z = np.dot(self.cache['A'][-1], weights) + biases\n",
    "            A = self.activations_funcs[self.activations[i]][0](z)\n",
    "            self.cache['A'].append(A)\n",
    "        return self.cache['A'][-1]\n",
    "\n",
    "    def backward(self, y_true, loss_type='mse'):\n",
    "        # Calculate initial error based on the loss type\n",
    "        A_final = self.cache['A'][-1]\n",
    "        error = (A_final - y_true) * (1 if loss_type == 'mse' else A_final * (1 - A_final))\n",
    "        \n",
    "        # Initialize gradients\n",
    "        gradients = []\n",
    "        \n",
    "        # Loop through layers in reverse order\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            weights, biases = self.layers[i]\n",
    "            A_prev = self.cache['A'][i]\n",
    "            dZ = error * self.activations_funcs[self.activations[i]][1](self.cache['A'][i+1])\n",
    "            dW = np.dot(A_prev.T, dZ) / len(A_prev)\n",
    "            dB = np.sum(dZ, axis=0) / len(A_prev)\n",
    "            gradients.insert(0, (dW, dB))\n",
    "            if i > 0:  # Backpropagate the error\n",
    "                error = np.dot(dZ, weights.T)\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "# Network configuration\n",
    "layer_files = ['layer1.csv', 'layer2.csv', 'layer3.csv', 'layer4.csv', 'layer5.csv']\n",
    "activations = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"]\n",
    "nh = [5, 4, 3, 5, 1]\n",
    "\n",
    "# Initialize the network\n",
    "network = neural_net(nh, activations, layer_files)\n",
    "\n",
    "# Input vector from problem set 8\n",
    "x_input = np.array([[0.1, -0.2, 0.3, -0.4, 0.5]])\n",
    "\n",
    "# Target output\n",
    "y_target = np.array([[3]])\n",
    "\n",
    "# Forward propagation\n",
    "output = network.forward(x_input)\n",
    "\n",
    "# Compute gradients for MSE and Cross-Entropy losses\n",
    "mse_gradients = network.backward(y_target, 'mse')\n",
    "cross_entropy_gradients = network.backward(y_target, 'cross_entropy')\n",
    "\n",
    "# Output the gradients of the first hidden layer\n",
    "print(\"MSE Gradients (First Hidden Layer): Weights\\n\", mse_gradients[0][0])\n",
    "print(\"MSE Gradients (First Hidden Layer): Biases\\n\", mse_gradients[0][1])\n",
    "print(\"Cross-Entropy Gradients (First Hidden Layer): Weights\\n\", cross_entropy_gradients[0][0])\n",
    "print(\"Cross-Entropy Gradients (First Hidden Layer): Biases\\n\", cross_entropy_gradients[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is a form of regularization used to avoid overfitting when training a machine learning model, particularly in the context of iterative methods such gradient descent. The idea behind early stopping is where we monitor the model's performance on a validation set during the training process and then stop training when the model's performance on the validation set starts to degrade, even if the performance on the training set continues to improve.\n",
    "\n",
    "This is considered a form of \"free lunch\" in machine learning because it effectively helps in preventing overfitting without the need to explicitly alter the model's complexity, such as by adding regularization terms. The model is trained just enough to learn the underlying patterns but not too much that it starts to learn the noise in the training data, which we wants to prevent. By using a portion of the data as a validation set, early stopping also leverages this data for tuning the number of training epochs, thus optimizing both the model's capacity and its generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [ 0.52102641  0.05276656 -0.03470804  0.02982283 -0.02577108  0.03492524]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sgd_with_early_stopping(func, grad, data, batch_size, initial_params, patience):\n",
    "    # Split data into training, validation, and test sets\n",
    "    train_data, temp_data = train_test_split(data, test_size=0.5, random_state=42)\n",
    "    valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Convert data to numpy arrays for easier manipulation\n",
    "    train_x = train_data.drop('y', axis=1).values\n",
    "    train_y = train_data['y'].values\n",
    "    valid_x = valid_data.drop('y', axis=1).values\n",
    "    valid_y = valid_data['y'].values\n",
    "\n",
    "    # Initialize parameters\n",
    "    params = np.array(initial_params, dtype=float)\n",
    "    best_validation_error = np.inf\n",
    "    best_params = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    while patience_counter < patience:\n",
    "        # Shuffle training data at the beginning of each epoch\n",
    "        indices = np.arange(train_x.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        train_x = train_x[indices]\n",
    "        train_y = train_y[indices]\n",
    "\n",
    "        # Perform SGD updates\n",
    "        for start in range(0, len(train_x), batch_size):\n",
    "            end = min(start + batch_size, len(train_x))\n",
    "            x_batch = train_x[start:end]\n",
    "            y_batch = train_y[start:end]\n",
    "            params -= 0.01 * grad(params, x_batch, y_batch)  # Update with gradient\n",
    "\n",
    "        # Evaluate validation error\n",
    "        validation_error = func(params, valid_x, valid_y)\n",
    "        if validation_error < best_validation_error:\n",
    "            best_validation_error = validation_error\n",
    "            best_params = params.copy()\n",
    "            patience_counter = 0  # Reset counter\n",
    "        else:\n",
    "            patience_counter += 1  # Increment counter as no improvement\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Example of a loss function and its gradient\n",
    "def loss_function(params, x, y):\n",
    "    predictions = x.dot(params)\n",
    "    return np.mean((predictions - y) ** 2)\n",
    "\n",
    "def gradient_function(params, x, y):\n",
    "    predictions = x.dot(params)\n",
    "    return 2 * x.T.dot(predictions - y) / len(y)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('ps9.csv')\n",
    "initial_params = [0.01] * (data.shape[1])  # One parameter per feature plus intercept\n",
    "\n",
    "# Adding an intercept column\n",
    "data['Intercept'] = 1\n",
    "features = ['Intercept'] + [col for col in data if col != 'y' and col != 'Intercept']\n",
    "\n",
    "# Adjust the initial parameters for the intercept\n",
    "initial_params = np.random.randn(len(features))\n",
    "\n",
    "# Use the function\n",
    "best_params = sgd_with_early_stopping(loss_function, gradient_function, data[features + ['y']], 32, initial_params, 10)\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting appropriate initial values for the weights and biases in a neural network is crucial for effective training. The choice of these initial values can significantly impact the speed of convergence during training as well as the ability of the network to reach a good generalization performance. Here are some considerations to take into account:\n",
    "\n",
    "- Avoid Symmetry Breaking: If all weights are initialized to the same value (such as zero), each neuron in a layer will learn the same features during training, which is ineffective. Random initialization helps break symmetry and ensures neurons can learn different functions.\n",
    "\n",
    "- Control Variance: The initial weights should be set so that the variance of the outputs from each layer is neither too high nor too low. Too high a variance can lead to exploding gradients, while too low a variance can cause vanishing gradients, especially with deep networks.\n",
    "\n",
    "# Activation Function Compatibility:\n",
    "\n",
    "ReLU (Rectified Linear Unit): Weights for layers using ReLU activation should be initialized in a way that reduces the likelihood of dead neurons (neurons that only output zero). A popular method is the He initialization, which sets the weights with variance scaled according to the number of incoming nodes, improving the flow of gradients.\n",
    "Sigmoid/Tanh: For these activations, it's vital to maintain a small range of variance at initialization to prevent saturation. Xavier/Glorot initialization is commonly used, where weights are initialized based on the number of input and output nodes to keep the variance stable across layers.\n",
    "Bias Initialization: Biases can generally be initialized to zero since the asymmetry breaking is primarily handled by the random weights. However, sometimes setting them to a small constant value like 0.01 can prevent neurons from being dead at the start, especially for ReLU activations.\n",
    "\n",
    "# Example of Good Initial Values for the Given Network Structure:\n",
    "Given the network structure from Problem Set 8 with nl = 5, nh = (5, 4, 3, 5, 1), and various activations (ReLU and sigmoid), we can choose appropriate initialization methods:\n",
    "\n",
    "For layers with ReLU activation, use He initialization:\n",
    "$$\n",
    "\\text{Var}(W) = \\frac{2}{\\text{number of input nodes}}\n",
    "$$\n",
    "\n",
    "For layers with sigmoid activation, use Xavier/Glorot initialization:\n",
    "$$\n",
    "\\text{Var}(W) = \\frac{1}{\\text{average of input and output nodes}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: shape (4, 5), first entry 0.31414961391137586\n",
      "b1: shape (4, 1), first entry 0.0\n",
      "W2: shape (3, 4), first entry 0.5539631645620577\n",
      "b2: shape (3, 1), first entry 0.0\n",
      "W3: shape (5, 3), first entry -0.01102043785053617\n",
      "b3: shape (5, 1), first entry 0.0\n",
      "W4: shape (1, 5), first entry 0.4315683416652254\n",
      "b4: shape (1, 1), first entry 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims, activations):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        if activations[l-1] == 'ReLU':\n",
    "            std_dev = np.sqrt(2. / layer_dims[l-1])  # He initialization\n",
    "        elif activations[l-1] == 'sigmoid':\n",
    "            std_dev = np.sqrt(1. / (layer_dims[l-1] + layer_dims[l]))  # Xavier initialization\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * std_dev\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Define network parameters\n",
    "layer_dims = [5, 4, 3, 5, 1]  # 5 layers with respective sizes\n",
    "activations = ['ReLU', 'sigmoid', 'ReLU', 'sigmoid', 'ReLU']  # Activation functions per layer\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = initialize_parameters(layer_dims, activations)\n",
    "\n",
    "for key, value in parameters.items():\n",
    "    print(f\"{key}: shape {value.shape}, first entry {value.flat[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would normalize the data for neural networks for the following reasons:\n",
    "\n",
    "1. **Faster Convergence**: Normalizing the data (for example, scaling input features to have zero mean and unit variance) helps in speeding up the learning process. It ensures that the gradient descent algorithm, which is often used for training neural networks, converges faster.\n",
    "\n",
    "2. **Balanced Feature Influence**: When features are on different scales, larger-scale features might dominate the learning process, potentially leading to a model that does not appropriately learn from other features. Normalization mitigates this risk by ensuring all features contribute equally to the model's learning.\n",
    "\n",
    "3. **Improved Numeric Stability**: Many activation functions used in neural networks, like the sigmoid or tanh, are sensitive to very large or very small inputs (leading to issues like vanishing or exploding gradients). Normalizing inputs helps avoid such extremes, maintaining numerical stability.\n",
    "\n",
    "Regarding penalization, which is typically implemented in the form of regularization (like L1, L2 regularization), the primary considerations are:\n",
    "\n",
    "1. **Weights Penalization**:\n",
    "   - Typically, only weights are penalized, not biases. Penalizing weights helps prevent overfitting by discouraging large weights, thus simplifying the model. A smaller weight magnitude generally leads to a smoother model where the output changes more slowly with changes in input, enhancing the model's generalization capabilities. The rationale behind this is that weights in a neural network control the magnitude of the contribution of inputs and the activation of neurons. Large weights can lead to a model that is overly complex and sensitive to small changes in input (high variance), capturing noise rather than the underlying data pattern.\n",
    "\n",
    "2. **Bias Penalization**:\n",
    "   - Bias terms are usually not penalized. This is because biases merely shift the activation function to the left or right, which helps the model fit better with less dependency on the specific distribution and scale of the inputs. Penalizing biases can unnecessarily restrict the model’s flexibility to fit the data, especially if the data itself is not centered or standardized.\n",
    "\n",
    "Therefore, for neural networks, we think it is generally better to penalize only the weights to maintain the model's capacity to adapt its learning to the data's mean structure while avoiding overfitting. Penalizing weights typically involves techniques like L2 regularization (Ridge), which adds a penalty equal to the square of the magnitude of coefficients.\n",
    "\n",
    "We believe that normalizing input data and selectively applying penalization to weights but not to biases provides a balanced approach to designing neural networks that are both robust and capable of generalizing well from training data to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
